{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Feature Extraction and Pre-Processing the training and the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cars_train.tgz...\n",
      "Training dataset extracted!\n",
      "Extracting cars_test.tgz...\n",
      "Testing dataset extracted!\n",
      "Extracting car_devkit.tgz...\n",
      "Metadata extracted!\n",
      "class_names.shape: (196, 1)\n",
      "Random class_name: [smart fortwo Convertible 2012]\n",
      "Preprocessing training dataset\n",
      "There are 196 number of car classifications available in this dataset!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (17 of 8144) |                      | Elapsed Time: 0:00:00 ETA:   0:01:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (8144 of 8144) |####################| Elapsed Time: 0:01:46 Time:  0:01:46\n",
      "N/A% (0 of 8041) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing testing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (8041 of 8041) |####################| Elapsed Time: 0:01:37 Time:  0:01:37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tarfile\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2 as cv\n",
    "import shutil\n",
    "import random\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "\n",
    "# Module to save the pre-processed training dataset\n",
    "def save_trainset(g_fname, g_label, g_bbox):\n",
    "    orig_train_folder = 'cars_train'\n",
    "    samples = len(g_fname) #Gets the number of samples by counting the length of the filename list\n",
    "\n",
    "    train_split = 0.8 # Splitting based on Pareto principle, where 80% sample used for training and 20% used for validation\n",
    "    num_train = int(round(samples * train_split))\n",
    "    train_indexes = random.sample(range(samples), num_train)\n",
    "    \n",
    "    #Creating a progressbar to track the progress of saving the dataset\n",
    "    pb = ProgressBar()\n",
    "    \n",
    "    #Looping through for all the samples\n",
    "    for i in pb(range(samples)):\n",
    "        fname = g_fname[i]\n",
    "        label = g_label[i]\n",
    "        (x1, y1, x2, y2) = g_bbox[i]\n",
    "\n",
    "        #sets the path as cars_train/filename of each image to create folder that contains similar images\n",
    "        orig_path = os.path.join(orig_train_folder, fname)\n",
    "        orig_image = cv.imread(orig_path) #Read the image\n",
    "        height, width = orig_image.shape[:2] #And its features\n",
    "\n",
    "        margin = 16 # Setting the margin as 16 so as to set the bounding box values to resize\n",
    "        x1 = max(0, x1 - margin)\n",
    "        y1 = max(0, y1 - margin)\n",
    "        x2 = min(x2 + margin, width)\n",
    "        y2 = min(y2 + margin, height)\n",
    "        \n",
    "        if i in train_indexes:\n",
    "            temp_folder = 'data/train'\n",
    "        else:\n",
    "            temp_folder = 'data/valid'\n",
    "        #sets path as data/train or data/valid based on the above condition and stores the resized image in the data folders after finding the similar images corresponding to each image\n",
    "        temp_path = os.path.join(temp_folder, label)\n",
    "        if not os.path.exists(temp_path):\n",
    "            os.makedirs(temp_path)\n",
    "        temp_path = os.path.join(temp_path, fname)\n",
    "\n",
    "        resize_image = orig_image[y1:y2, x1:x2]\n",
    "        temp_img = cv.resize(src=resize_image, dsize=(ih, iw))\n",
    "        cv.imwrite(temp_path, temp_img)\n",
    "\n",
    "# Module to save the preprocessed test dataset\n",
    "# Logic is pretty much the same as the training dataset\n",
    "def save_testset(g_fname, g_bbox):\n",
    "    orig_folder = 'cars_test'\n",
    "    temp_folder = 'data/test'\n",
    "    samples = len(g_fname)\n",
    "    pb = ProgressBar()\n",
    "    for i in pb(range(samples)):\n",
    "        fname = g_fname[i]\n",
    "        (x1, y1, x2, y2) = g_bbox[i]\n",
    "        orig_path = os.path.join(orig_folder, fname)\n",
    "        \n",
    "        orig_image = cv.imread(orig_path)\n",
    "        height, width = orig_image.shape[:2]\n",
    "        \n",
    "        margin = 16\n",
    "        x1 = max(0, x1 - margin)\n",
    "        y1 = max(0, y1 - margin)\n",
    "        x2 = min(x2 + margin, width)\n",
    "        y2 = min(y2 + margin, height)\n",
    "        \n",
    "        temp_path = os.path.join(temp_folder, fname)\n",
    "        resize_image = orig_image[y1:y2, x1:x2]\n",
    "        temp_img = cv.resize(src=resize_image, dsize=(ih, iw))\n",
    "        cv.imwrite(temp_path, temp_img)\n",
    "\n",
    "# Module to preprocess the training data set\n",
    "def preprocess_trainset():\n",
    "    print(\"Preprocessing training dataset\")\n",
    "    # Get the metadata about the dataset annotations and store, transpose it to get the actual features required\n",
    "    g_cannos = scipy.io.loadmat('devkit/cars_train_annos')\n",
    "    g_annotations = g_cannos['annotations']\n",
    "    g_annotations = np.transpose(g_annotations)\n",
    "\n",
    "    g_fname = [] # List to store the filenames associated with each image\n",
    "    g_class_id = [] # List to store class ids of the class that the image belongs to\n",
    "    g_bbox = [] # List to store bounding box values of each image\n",
    "    g_label = [] # List to store the label associated with each image\n",
    "\n",
    "    #Looping in the annotations to split the features and store in the corresponding lists as defined above    \n",
    "    for g_annotation in g_annotations:\n",
    "        bbox_x1 = g_annotation[0][0][0][0]\n",
    "        bbox_y1 = g_annotation[0][1][0][0]\n",
    "        bbox_x2 = g_annotation[0][2][0][0]\n",
    "        bbox_y2 = g_annotation[0][3][0][0]\n",
    "        class_id = g_annotation[0][4][0][0]\n",
    "        g_label.append('%04d' % (class_id,))\n",
    "        fname = g_annotation[0][5][0]\n",
    "        g_bbox.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n",
    "        g_class_id.append(class_id)\n",
    "        g_fname.append(fname)\n",
    "\n",
    "    g_lcount = np.unique(g_class_id).shape[0] # Getting the unique class ids to count the number of classifications available\n",
    "    \n",
    "    print('There are %d number of car classifications available in this dataset!' % g_lcount)\n",
    "    \n",
    "    #Saving the preprocessed training dataset\n",
    "    save_trainset(g_fname, g_label, g_bbox)\n",
    "\n",
    "# Module to preprocess the test data set\n",
    "def preprocess_testset():\n",
    "    print(\"Preprocessing testing dataset...\")\n",
    "    #Getting the filename and image information from the metadata extracted folder using scipy\n",
    "    g_cannos = scipy.io.loadmat('devkit/cars_test_annos')\n",
    "    g_annotations = g_cannos['annotations']\n",
    "    g_annotations = np.transpose(g_annotations) # Transposing it to get the actual features\n",
    "    \n",
    "    g_fname = [] #List to store the filename associated with each of the image\n",
    "    g_bbox = [] #List to store the bounding box values for each image - x1, y1, x2, y2 values\n",
    "\n",
    "    #Loop through to fetch the values into the corresponding lists\n",
    "    for g_annotation in g_annotations:\n",
    "        bbox_x1 = g_annotation[0][0][0][0]\n",
    "        bbox_y1 = g_annotation[0][1][0][0]\n",
    "        bbox_x2 = g_annotation[0][2][0][0]\n",
    "        bbox_y2 = g_annotation[0][3][0][0]\n",
    "        fname = g_annotation[0][4][0]\n",
    "        g_bbox.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n",
    "        g_fname.append(fname)\n",
    "       \n",
    "    # Saving the preprocessed test dataset\n",
    "    save_testset(g_fname, g_bbox)\n",
    "\n",
    "# Module to check the folders for training, validation and test datasets    \n",
    "def check_folder(folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "#As an entry point, the below module covers the process of extracting the datasets and calling the preprocess modules\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parameters - width and height of the images\n",
    "    iw, ih = 224, 224\n",
    "    \n",
    "    #Extracting the training dataset into cars_train folder    \n",
    "    print('Extracting cars_train.tgz...')\n",
    "    if not os.path.exists('cars_train'):\n",
    "        with tarfile.open('cars_train.tgz', \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "    print('Training dataset extracted!')\n",
    "    \n",
    "    #Extracting the test dataset into the cars_test folder\n",
    "    print('Extracting cars_test.tgz...')\n",
    "    if not os.path.exists('cars_test'):\n",
    "        with tarfile.open('cars_test.tgz', \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "    print('Testing dataset extracted!')\n",
    "    \n",
    "    #Extracting the devkit folder that has metadata about the images into the devkit folder\n",
    "    print('Extracting car_devkit.tgz...')\n",
    "    if not os.path.exists('devkit'):\n",
    "        with tarfile.open('car_devkit.tgz', \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "    print('Metadata extracted!')\n",
    "    \n",
    "    #Extracting the metadata from mat file using the scipy lib and storing in the python variables\n",
    "    g_cmeta = scipy.io.loadmat('devkit\\cars_meta')    \n",
    "    class_names = g_cmeta['class_names']  \n",
    "    #Getting the class_names values from the mat file, to store the car classifications and transposing the class_names variable to get the class_names shape\n",
    "    class_names = np.transpose(class_names)\n",
    "    print('class_names.shape: ' + str(class_names.shape))\n",
    "    print('Random class_name: [{}]'.format(class_names[195][0][0]))\n",
    "    \n",
    "    #Checks if the folder exists, if not create the folders in order to use it for training and testing\n",
    "    check_folder('data/train')\n",
    "    check_folder('data/valid')\n",
    "    check_folder('data/test')\n",
    "\n",
    "    preprocess_trainset()\n",
    "    preprocess_testset()\n",
    "\n",
    "    # clean up the folders for better memory usage and performance\n",
    "    # shutil.rmtree('cars_train')\n",
    "    shutil.rmtree('cars_test') # cleaning just the test dataset folder, as I just started with training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
